# Playbook de Engenharia com IA

**Vers√£o:** 1.0  
**Data:** Novembro 2025  
**Prop√≥sito:** Guia pr√°tico para usar IA em sprints de evolu√ß√£o de software

---

## üìñ √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Fases/Sprints Recomendadas](#fases-sprints-recomendadas)
3. [Boas Pr√°ticas de Uso de IA](#boas-pr√°ticas-de-uso-de-ia)
4. [Exemplos de Prompts](#exemplos-de-prompts)
5. [Checklists de Production-Readiness](#checklists-de-production-readiness)
6. [Ferramentas e Recursos](#ferramentas-e-recursos)
7. [Solu√ß√£o de Problemas](#solu√ß√£o-de-problemas)

---

## üéØ Vis√£o Geral

### O M√©todo: Sprints com IA

Este playbook documenta um m√©todo comprovado para evoluir projetos de software usando IA como acelerador. A abordagem consiste em:

1. **Dividir evolu√ß√£o em sprints tem√°ticas** (estrutura, testes, observabilidade, seguran√ßa, etc.)
2. **Usar IA para diagnosticar, planejar e implementar** cada sprint
3. **Validar continuamente** com testes automatizados e scans de seguran√ßa
4. **Documentar aprendizados** em relat√≥rios de sprint reutiliz√°veis

### Quando Usar Este M√©todo

‚úÖ **Cen√°rios Ideais:**
- Reposit√≥rios com c√≥digo funcional mas desorganizado
- Projetos com baixa cobertura de testes (< 60%)
- Sistemas sem observabilidade adequada
- Aplica√ß√µes que precisam de hardening de seguran√ßa
- Prepara√ß√£o para ambientes de produ√ß√£o

‚ùå **N√£o Recomendado Para:**
- Projetos greenfield (come√ßando do zero)
- C√≥digo legado sem testes e sem entendimento
- Sistemas cr√≠ticos em produ√ß√£o sem ambiente de staging
- Prazos muito apertados (< 1 semana por sprint)

### Benef√≠cios Esperados

Baseado na experi√™ncia do projeto 3dPot:

| M√©trica | Antes | Depois (9 Sprints) | Ganho |
|---------|-------|-------------------|-------|
| **Cobertura de Testes** | 40% | 85% | +45% |
| **Testes Totais** | 93 | 748 | +655 |
| **Observabilidade** | B√°sica | Avan√ßada | Logs + Metrics + Trace |
| **Seguran√ßa** | JWT | JWT + Rate + Audit + RBAC + MFA | 5+ camadas |
| **Production-Ready** | 40% | 98% | +58% |
| **Tempo Investido** | - | ~9 sprints | 2-4 semanas |

### Princ√≠pios Fundamentais

1. **Incrementalismo Controlado**: Cada sprint adiciona uma camada espec√≠fica
2. **Valida√ß√£o Cont√≠nua**: Testes e scans em cada mudan√ßa
3. **Zero Regress√µes**: Mudan√ßas sempre aditivas e retrocompat√≠veis
4. **IA como Colaboradora**: IA sugere, humano valida
5. **Documenta√ß√£o Viva**: Cada sprint documenta seu impacto

---

## üèóÔ∏è Fases/Sprints Recomendadas

### Fase 1: Funda√ß√£o (Sprints 1-2)

**Objetivo:** Estabelecer base s√≥lida de estrutura e testes

#### Sprint 1: Reorganiza√ß√£o e Estrutura
- **Foco:** Organizar reposit√≥rio em estrutura naveg√°vel
- **Dura√ß√£o:** 1-2 dias
- **Entregas:** Diret√≥rios organizados, README atualizado, STRUCTURE.md
- **Impacto:** +80% navegabilidade, onboarding mais f√°cil

#### Sprint 2: Testes B√°sicos
- **Foco:** Cobrir m√≥dulos cr√≠ticos com testes unit√°rios
- **Dura√ß√£o:** 3-5 dias
- **Entregas:** 150-200 testes, cobertura 70%+
- **Impacto:** +30-40% cobertura, confian√ßa em refatora√ß√µes

**Checkpoint:** Voc√™ tem c√≥digo organizado e test√°vel? ‚úÖ Prossiga para Fase 2

---

### Fase 2: Consolida√ß√£o (Sprints 3-5)

**Objetivo:** Expandir cobertura e estabelecer ferramentas

#### Sprint 3: Integra√ß√£o + CLI
- **Foco:** Consolidar testes de integra√ß√£o, criar CLI
- **Dura√ß√£o:** 2-3 dias
- **Entregas:** CLI com 8-10 comandos, 5-10 testes E2E
- **Impacto:** Ferramentas de dev, testes de fluxo completo

#### Sprint 4: Cobertura + CI
- **Foco:** M√≥dulos secund√°rios, fortalecer CI/CD
- **Dura√ß√£o:** 3-4 dias
- **Entregas:** 80-120 testes, CI com coverage
- **Impacto:** +10% cobertura, CI automatizado

#### Sprint 5: Qualidade Final
- **Foco:** 100% servi√ßos com testes, performance baseline
- **Dura√ß√£o:** 2-3 dias
- **Entregas:** Todos servi√ßos testados, benchmarks
- **Impacto:** 85%+ cobertura, baseline de performance

**Checkpoint:** Voc√™ tem 85%+ cobertura e CI funcional? ‚úÖ Prossiga para Fase 3

---

### Fase 3: Production-Ready (Sprints 6-9)

**Objetivo:** Preparar para produ√ß√£o com observabilidade e seguran√ßa

#### Sprint 6: Observabilidade
- **Foco:** Logging estruturado, m√©tricas, tracing
- **Dura√ß√£o:** 2-3 dias
- **Entregas:** Logs JSON, /metrics, request_id
- **Impacto:** Debugging eficiente, monitoramento

#### Sprint 7: Seguran√ßa Base
- **Foco:** Rate limiting, audit logging, RBAC
- **Dura√ß√£o:** 2-3 dias
- **Entregas:** Rate limiter, audit logs, RBAC
- **Impacto:** Prote√ß√£o contra abuso, rastreabilidade

#### Sprint 8: Hardening
- **Foco:** Escala horizontal, security gates
- **Dura√ß√£o:** 2-3 dias
- **Entregas:** Rate limiting distribu√≠do, CI security
- **Impacto:** Escalabilidade, pipeline seguro

#### Sprint 9: Opera√ß√µes + DR
- **Foco:** MFA, disaster recovery, runbook
- **Dura√ß√£o:** 3-4 dias
- **Entregas:** MFA/2FA, scripts DR, runbook 500+ linhas
- **Impacto:** 98% production-ready

**Checkpoint:** Voc√™ tem 98%+ production-ready? ‚úÖ Deploy para produ√ß√£o!

---

### Sequ√™ncia Visual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE 1: FUNDA√á√ÉO                     ‚îÇ
‚îÇ  Sprint 1 (Estrutura) ‚Üí Sprint 2 (Testes B√°sicos)      ‚îÇ
‚îÇ  Ganho: +30-40% cobertura, c√≥digo organizado           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 FASE 2: CONSOLIDA√á√ÉO                    ‚îÇ
‚îÇ  Sprint 3 (CLI) ‚Üí Sprint 4 (CI) ‚Üí Sprint 5 (Qualidade) ‚îÇ
‚îÇ  Ganho: +15% cobertura, CI automatizado, CLI           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               FASE 3: PRODUCTION-READY                  ‚îÇ
‚îÇ  Sprint 6 (Obs) ‚Üí Sprint 7 (Sec) ‚Üí Sprint 8 (Hard)     ‚îÇ
‚îÇ  ‚Üí Sprint 9 (Ops+DR)                                    ‚îÇ
‚îÇ  Ganho: Observabilidade + Seguran√ßa + Ops              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                  üöÄ PRODUCTION READY (98%)
```

---

## ü§ñ Boas Pr√°ticas de Uso de IA

### 1. Como Escrever Prompts Eficazes

#### Estrutura de Prompt Recomendada

```
[PAPEL DA IA]
Voc√™ √© um [especialista em X].

[CONTEXTO]
- Reposit√≥rio: [URL]
- Stack: [tecnologias]
- Estado atual: [m√©tricas]

[PROBLEMA]
[Descri√ß√£o clara do problema]

[OBJETIVO]
[O que voc√™ quer alcan√ßar]

[TAREFAS QUE VOC√ä DEVE EXECUTAR]
1. [Tarefa espec√≠fica 1]
2. [Tarefa espec√≠fica 2]
...

[FORMATO DE SA√çDA]
1. [O que voc√™ espera receber]
2. [Formato preferido]

[RESTRI√á√ïES]
- N√ÉO [a√ß√£o indesejada]
- SEMPRE [a√ß√£o obrigat√≥ria]

[M√âTRICAS DE SUCESSO]
- [Crit√©rio 1]
- [Crit√©rio 2]
```

#### Exemplo de Prompt Eficaz

```
Voc√™ √© um especialista em testes de software.

[CONTEXTO]
- Reposit√≥rio: https://github.com/user/project
- Stack: Python/FastAPI
- Cobertura atual: 40%

[PROBLEMA]
O servi√ßo budgeting_service.py (~500 linhas) n√£o tem testes unit√°rios.

[OBJETIVO]
Criar 40-50 testes unit√°rios cobrindo happy paths, edge cases e error handling.

[TAREFAS]
1. Analisar budgeting_service.py e identificar m√©todos p√∫blicos
2. Criar test_budgeting_service.py seguindo conven√ß√£o pytest
3. Implementar testes para cada m√©todo p√∫blico
4. Usar mocks para depend√™ncias externas (API Octopart)
5. Validar que todos os testes passam

[FORMATO DE SA√çDA]
1. Arquivo test_budgeting_service.py completo
2. Resumo de testes criados (nome, prop√≥sito)
3. Comandos para executar testes

[RESTRI√á√ïES]
- N√ÉO modificar budgeting_service.py
- N√ÉO criar testes que dependam de APIs externas reais
- USAR mocks/stubs apropriados

[M√âTRICAS]
- 40+ testes criados
- Todos os m√©todos p√∫blicos cobertos
- Tempo de execu√ß√£o < 5 segundos
```

### 2. Como Interpretar Respostas da IA

#### Sinais de Boa Resposta ‚úÖ

- **Espec√≠fica:** C√≥digo concreto, n√£o pseudoc√≥digo gen√©rico
- **Completa:** Cobre todos os pontos solicitados
- **Test√°vel:** Inclui exemplos de uso ou testes
- **Documentada:** Explica decis√µes e trade-offs
- **Contextual:** Considera stack e conven√ß√µes do projeto

#### Sinais de Resposta Problem√°tica ‚ö†Ô∏è

- **Gen√©rica:** "Voc√™ poderia fazer X ou Y..."
- **Incompleta:** Falta partes importantes do c√≥digo
- **N√£o test√°vel:** Sem exemplos ou valida√ß√£o
- **Sem contexto:** Ignora stack ou estrutura existente
- **Insegura:** Sugere pr√°ticas inseguras (secrets hardcoded, etc.)

#### O Que Fazer Com Respostas Problem√°ticas

1. **Refinar o Prompt:** Adicione mais contexto ou seja mais espec√≠fico
2. **Dividir em Partes:** Se muito complexo, divida em tarefas menores
3. **Dar Exemplos:** Mostre c√≥digo existente como refer√™ncia
4. **Iterar:** Use a resposta como ponto de partida e refine

### 3. Como Validar com Testes e Scans

#### Valida√ß√£o Obrigat√≥ria Ap√≥s Cada Sprint

```bash
# 1. Testes Unit√°rios
pytest tests/unit/ -v
# ‚úÖ Todos devem passar

# 2. Testes de Integra√ß√£o
pytest tests/integration/ -v
# ‚úÖ Todos devem passar

# 3. Testes E2E (se aplic√°vel)
pytest tests/e2e/ -v
# ‚úÖ Todos devem passar

# 4. Coverage
pytest --cov=src --cov-report=html --cov-fail-under=70
# ‚úÖ >= threshold configurado

# 5. Linting
pylint src/
# ‚úÖ Score >= 8.0

# 6. Type Checking (se aplic√°vel)
mypy src/
# ‚úÖ 0 erros

# 7. Security Scan
bandit -r src/ -ll
# ‚úÖ 0 issues cr√≠ticos

# 8. Dependency Scan
pip-audit --desc
# ‚úÖ 0 vulnerabilidades cr√≠ticas n√£o mitigadas
```

#### Checklist de Valida√ß√£o

```markdown
## Valida√ß√£o P√≥s-Sprint

### Funcional
- [ ] Todos os testes passam (unit + integration + e2e)
- [ ] Build/run funciona sem erros
- [ ] Funcionalidades existentes n√£o quebradas

### Qualidade
- [ ] Coverage >= threshold
- [ ] Linting sem erros cr√≠ticos
- [ ] Type checking sem erros (se aplic√°vel)
- [ ] C√≥digo segue conven√ß√µes do projeto

### Seguran√ßa
- [ ] 0 secrets hardcoded
- [ ] 0 vulnerabilidades cr√≠ticas (Bandit/SAST)
- [ ] 0 vulnerabilidades cr√≠ticas (pip-audit/Safety)
- [ ] Depend√™ncias atualizadas

### Documenta√ß√£o
- [ ] README atualizado (se aplic√°vel)
- [ ] Docstrings para novos m√©todos p√∫blicos
- [ ] Relat√≥rio de sprint criado
- [ ] Changelog atualizado (se aplic√°vel)
```

### 4. Itera√ß√£o e Refinamento

#### Ciclo de Feedback

```
1. Escrever Prompt ‚Üí 2. IA Gera C√≥digo ‚Üí 3. Revisar C√≥digo
                          ‚Üë                        ‚Üì
                    6. Validar ‚Üê 5. Aplicar ‚Üê 4. Refinar
                          ‚Üì
                    7. Documentar
```

#### Quando Iterar

- **Primeira tentativa:** Sempre revise e valide
- **Se testes falham:** Refine o prompt ou corrija manualmente
- **Se c√≥digo n√£o segue padr√µes:** Adicione exemplos ao prompt
- **Se resposta gen√©rica:** Seja mais espec√≠fico no prompt

#### Como Iterar Eficientemente

1. **Mantenha hist√≥rico:** Salve prompts que funcionaram
2. **Documente problemas:** Anote o que n√£o funcionou e por qu√™
3. **Construa biblioteca:** Crie biblioteca de prompts reutiliz√°veis
4. **Compartilhe:** Compartilhe prompts eficazes com a equipe

---

## üìù Exemplos de Prompts

### Exemplo 1: Prompt Ruim ‚Üí Bom

#### ‚ùå Prompt Ruim

```
Crie testes para o meu servi√ßo de autentica√ß√£o.
```

**Problemas:**
- Muito gen√©rico
- Sem contexto (stack, framework)
- Sem especificar o que testar
- Sem restri√ß√µes
- Sem crit√©rios de sucesso

#### ‚úÖ Prompt Bom

```
Voc√™ √© um especialista em testes de seguran√ßa.

[CONTEXTO]
- Reposit√≥rio: https://github.com/user/api
- Stack: Python/FastAPI
- Autentica√ß√£o: JWT com refresh tokens
- Arquivo: backend/services/auth_service.py (300 linhas)

[OBJETIVO]
Criar 30-40 testes unit√°rios para auth_service.py cobrindo:
- Login (sucesso, falha, credenciais inv√°lidas)
- Registro (sucesso, usu√°rio duplicado, valida√ß√µes)
- Gera√ß√£o de tokens (access + refresh)
- Valida√ß√£o de tokens (v√°lido, expirado, inv√°lido)
- Refresh de tokens
- Logout

[TAREFAS]
1. Analisar auth_service.py
2. Criar test_auth_service.py usando pytest
3. Implementar testes com fixtures para usu√°rios de teste
4. Usar freezegun para controlar tempo (testar expira√ß√£o)
5. Mockar hash de senhas e JWT encoding/decoding

[FORMATO DE SA√çDA]
1. Arquivo test_auth_service.py completo
2. Fixtures necess√°rias
3. Comando para executar: pytest tests/unit/services/test_auth_service.py -v

[RESTRI√á√ïES]
- N√ÉO usar banco de dados real (usar mocks/fixtures)
- N√ÉO expor senhas em logs de teste
- SEMPRE testar tanto sucesso quanto falha
- Testes devem ser determin√≠sticos (sem randomness)

[M√âTRICAS]
- 30-40 testes criados
- Cobertura do auth_service >= 85%
- Tempo de execu√ß√£o < 3 segundos
```

**Por que √© melhor:**
- Contexto completo (stack, framework, arquivo)
- Objetivo espec√≠fico (30-40 testes, cen√°rios claros)
- Tarefas detalhadas (an√°lise, fixtures, mocks)
- Formato de sa√≠da claro
- Restri√ß√µes de seguran√ßa
- M√©tricas mensur√°veis

---

### Exemplo 2: Prompt para Observabilidade

#### ‚úÖ Prompt Eficaz

```
Voc√™ √© um especialista em observabilidade de sistemas distribu√≠dos.

[CONTEXTO]
- Reposit√≥rio: https://github.com/user/backend
- Stack: Python 3.11 + FastAPI 0.104
- Logging atual: print() statements
- Objetivo: Production-ready observability

[PROBLEMA]
Sistema n√£o tem observabilidade adequada para produ√ß√£o. Logs n√£o estruturados, sem m√©tricas, sem correla√ß√£o de requisi√ß√µes.

[OBJETIVO]
Implementar observabilidade production-ready com:
1. Logging estruturado (JSON para prod, console para dev)
2. M√©tricas Prometheus (/metrics endpoint)
3. Request ID para correla√ß√£o

[TAREFAS]
1. Implementar logging estruturado:
   - Usar structlog
   - Formato JSON para produ√ß√£o
   - Formato console colorido para dev
   - Configurar via LOG_LEVEL e LOG_FORMAT env vars
   - Criar backend/observability/logging_config.py

2. Implementar request ID middleware:
   - Gerar UUID √∫nico por requisi√ß√£o
   - Adicionar header X-Request-ID
   - Propagar em todos os logs
   - Criar backend/observability/request_id.py

3. Implementar m√©tricas Prometheus:
   - Endpoint /metrics
   - M√©tricas HTTP: requests_total, duration_seconds, in_progress
   - Labels: method, endpoint, status
   - Criar backend/observability/metrics.py

4. Criar middleware de logging autom√°tico:
   - Logar todas as requisi√ß√µes
   - Incluir: method, path, status, duration, request_id
   - Logs especiais para 4xx/5xx
   - Excluir /health por padr√£o

5. Documentar:
   - Como configurar (env vars)
   - Exemplos de logs JSON
   - Exemplos de queries Prometheus
   - Criar docs/OBSERVABILITY.md

[FORMATO DE SA√çDA]
1. Arquivos criados:
   - backend/observability/__init__.py
   - backend/observability/logging_config.py
   - backend/observability/request_id.py
   - backend/observability/metrics.py
2. Exemplo de log JSON
3. Lista de m√©tricas expostas
4. Documenta√ß√£o OBSERVABILITY.md
5. Instru√ß√µes de integra√ß√£o no main.py

[RESTRI√á√ïES]
- N√ÉO logar dados sens√≠veis (passwords, tokens)
- N√ÉO logar health checks (muito ru√≠do)
- Configura√ß√£o via environment variables (.env)
- Suporte a desenvolvimento (logs leg√≠veis)
- Performance overhead < 5ms por requisi√ß√£o

[M√âTRICAS]
- Logs estruturados em JSON para prod
- Request ID em 100% dos logs
- /metrics retornando m√©tricas v√°lidas Prometheus
- Documenta√ß√£o completa
- 0 dados sens√≠veis em logs
```

---

### Exemplo 3: Prompt para Seguran√ßa

#### ‚úÖ Prompt Eficaz

```
Voc√™ √© um especialista em seguran√ßa de aplica√ß√µes web.

[CONTEXTO]
- Reposit√≥rio: https://github.com/user/api
- Stack: Python/FastAPI
- Autentica√ß√£o: JWT
- Observabilidade: Logs estruturados (Sprint 6)

[PROBLEMA]
API vulner√°vel a abuso (sem rate limiting), sem auditoria de a√ß√µes cr√≠ticas, controle de acesso b√°sico.

[OBJETIVO]
Implementar camada de seguran√ßa com:
1. Rate limiting (prote√ß√£o contra abuso)
2. Audit logging (rastreamento de a√ß√µes)
3. RBAC melhorado (controle de acesso granular)

[TAREFAS]
1. Rate Limiting (Token Bucket):
   - Algoritmo Token Bucket (permite bursts)
   - Limites por endpoint:
     - /auth/login: 10 req/min
     - /auth/register: 10 req/min
     - APIs caras: 30 req/min
     - Default: 60 req/min
   - Diferenciar IP e usu√°rio autenticado
   - Retornar 429 com headers: X-RateLimit-Limit, Remaining, Retry-After
   - Configur√°vel via env vars
   - Criar backend/observability/rate_limiting.py

2. Audit Logging:
   - Tabela audit_logs (n√£o deletar jamais)
   - Registrar:
     - Login/logout (sucesso/falha)
     - Mudan√ßas de permiss√µes
     - Acesso a recursos sens√≠veis
   - Campos: timestamp, user_id, action, resource, result, ip, user_agent, request_id
   - Integrar com logging estruturado
   - Criar backend/services/audit_service.py

3. RBAC:
   - Roles: USER, ADMIN
   - Decorator @require_role("admin")
   - Validar em endpoints sens√≠veis
   - Retornar 403 se sem permiss√£o
   - Logar tentativas de acesso negado

4. Testes:
   - 40+ testes de seguran√ßa
   - Rate limiting (hit limit, 429)
   - Audit logging (a√ß√µes registradas)
   - RBAC (autorizado/negado)

5. Documentar:
   - Pol√≠ticas de rate limiting
   - Como consultar audit logs
   - Estrutura de RBAC
   - Criar docs/SECURITY.md

[FORMATO DE SA√çDA]
1. Arquivos criados:
   - backend/observability/rate_limiting.py
   - backend/services/audit_service.py
   - backend/core/authorization.py
   - tests/unit/test_security.py
2. Instru√ß√µes de integra√ß√£o
3. Exemplos de audit logs
4. Documenta√ß√£o SECURITY.md

[RESTRI√á√ïES]
- N√ÉO expor informa√ß√µes em erros (user exists, etc.)
- Rate limiting n√£o deve afetar usu√°rios leg√≠timos
- Audit logs NUNCA modific√°veis/delet√°veis
- RBAC fail-safe (negar por padr√£o)
- 0 secrets hardcoded

[M√âTRICAS]
- Rate limiting retornando 429
- Audit trail de 100% a√ß√µes cr√≠ticas
- RBAC bloqueando acessos n√£o autorizados
- 40+ testes passando
- 0 secrets em c√≥digo
```

---

## ‚úÖ Checklists de Production-Readiness

### Checklist Geral (98% Production-Ready)

#### 1. C√≥digo e Arquitetura ‚úÖ

```markdown
- [ ] Estrutura de diret√≥rios clara e documentada
- [ ] Separa√ß√£o de concerns (models, services, routers, etc.)
- [ ] Configura√ß√£o via environment variables (.env)
- [ ] 0 secrets hardcoded
- [ ] Linting configurado e passando (score >= 8.0)
- [ ] Type hints (Python) ou TypeScript
- [ ] Docstrings/JSDoc para m√©todos p√∫blicos
```

#### 2. Testes ‚úÖ

```markdown
- [ ] Cobertura >= 85%
- [ ] Testes unit√°rios para todos os servi√ßos
- [ ] Testes de integra√ß√£o consolidados
- [ ] Testes E2E para fluxos cr√≠ticos (5-10 fluxos)
- [ ] Testes de CLI (se aplic√°vel)
- [ ] Testes de seguran√ßa (40+ testes)
- [ ] Testes executam em < 5 minutos
- [ ] 0 testes flakey (intermitentes)
- [ ] Mocks apropriados (sem depend√™ncias externas)
```

#### 3. Observabilidade ‚úÖ

```markdown
- [ ] Logging estruturado (JSON para prod, console para dev)
- [ ] Request ID em todos os logs
- [ ] Trace ID para distributed tracing
- [ ] M√©tricas Prometheus (/metrics)
- [ ] M√©tricas HTTP (requests, duration, errors)
- [ ] M√©tricas de neg√≥cio (se aplic√°vel)
- [ ] Health checks (/health, /healthz)
- [ ] Readiness checks (DB, Redis, etc.)
- [ ] 0 dados sens√≠veis em logs
```

#### 4. Seguran√ßa ‚úÖ

```markdown
- [ ] Autentica√ß√£o robusta (JWT + refresh tokens)
- [ ] Rate limiting por endpoint
- [ ] Rate limiting distribu√≠do (se multi-inst√¢ncia)
- [ ] Audit logging para a√ß√µes cr√≠ticas
- [ ] RBAC com valida√ß√£o de ownership
- [ ] MFA/2FA (opcional ou obrigat√≥rio)
- [ ] HTTPS obrigat√≥rio em produ√ß√£o
- [ ] CORS configurado adequadamente
- [ ] Headers de seguran√ßa (CSP, X-Frame-Options, etc.)
- [ ] Input validation e sanitization
- [ ] SQL injection protection (ORM)
- [ ] XSS protection
- [ ] CSRF protection (se stateful)
```

#### 5. CI/CD ‚úÖ

```markdown
- [ ] Pipeline automatizado (GitHub Actions, GitLab CI, etc.)
- [ ] Jobs separados (unit, integration, e2e, lint, coverage)
- [ ] Security gates:
  - [ ] SAST (Bandit, ESLint security, etc.)
  - [ ] Dependency scanning (Safety, npm audit, etc.)
  - [ ] Secret scanning (TruffleHog, etc.)
- [ ] Coverage reporting (Codecov, Coveralls)
- [ ] Badges de status no README
- [ ] Pol√≠tica de merge (CI must pass)
- [ ] Deploy automatizado (staging)
```

#### 6. Seguran√ßa - Scans ‚úÖ

```markdown
- [ ] SAST executado e documentado
- [ ] Dependency scan executado
- [ ] 0 vulnerabilidades cr√≠ticas n√£o mitigadas
- [ ] Vulnerabilidades m√©dias justificadas
- [ ] Security summary documentado
- [ ] Depend√™ncias cr√≠ticas atualizadas
```

#### 7. Opera√ß√µes ‚úÖ

```markdown
- [ ] Operations runbook (500+ linhas):
  - [ ] Detec√ß√£o de incidentes
  - [ ] Triagem inicial
  - [ ] Procedimentos de rollback
  - [ ] Investiga√ß√£o com audit logs
  - [ ] Troubleshooting comum
  - [ ] Checklist p√≥s-incidente
- [ ] Scripts de backup automatizados
- [ ] Scripts de restore validados
- [ ] Disaster recovery testado
- [ ] RPO < 24h, RTO < 30 min
- [ ] Procedimentos de escala√ß√£o documentados
```

#### 8. Documenta√ß√£o ‚úÖ

```markdown
- [ ] README completo e atualizado:
  - [ ] Quick start (5 minutos)
  - [ ] Instala√ß√£o e configura√ß√£o
  - [ ] Comandos principais
  - [ ] Exemplos de uso
  - [ ] Badges de status
- [ ] Estrutura documentada (STRUCTURE.md)
- [ ] Guia de contribui√ß√£o (CONTRIBUTING.md)
- [ ] Documenta√ß√£o de API (Swagger/OpenAPI)
- [ ] Runbook operacional
- [ ] Guia de observabilidade
- [ ] Guia de seguran√ßa
- [ ] Changelog atualizado
- [ ] Relat√≥rios de sprint
```

#### 9. Infraestrutura üîÑ

```markdown
- [ ] Containeriza√ß√£o (Dockerfile)
- [ ] Docker Compose para dev
- [ ] Orquestra√ß√£o (K8s manifests ou equivalente)
- [ ] Configura√ß√£o por ambiente (.env)
- [ ] Secrets management (Vault, AWS Secrets Manager, etc.)
- [ ] Monitoramento (Prometheus + Grafana)
- [ ] Alerting configurado (Alertmanager)
- [ ] Backups agendados (cron/systemd)
- [ ] Load balancing (se multi-inst√¢ncia)
- [ ] Auto-scaling (se necess√°rio)
```

#### 10. Compliance üîÑ (Opcional)

```markdown
- [ ] LGPD/GDPR compliance (se aplic√°vel)
- [ ] SOC 2 / ISO 27001 (se necess√°rio)
- [ ] Penetration testing externo
- [ ] Audit logs retention policy (90+ dias)
- [ ] Data encryption at rest
- [ ] Data encryption in transit (TLS)
- [ ] Privacy policy documentada
- [ ] Terms of service documentados
```

**Legenda:**
- ‚úÖ Implementado (baseado em 3dPot Sprint 9)
- üîÑ Parcial ou pr√≥ximos passos

---

### Checklist por Sprint

#### Sprint 1: Estrutura
```markdown
- [ ] Diret√≥rios organizados (src, tests, docs, scripts)
- [ ] Arquivos na raiz reduzidos (>70%)
- [ ] README atualizado
- [ ] STRUCTURE.md criado
- [ ] MIGRATION_GUIDE.md (se aplic√°vel)
- [ ] .gitignore atualizado
- [ ] Build/testes funcionando
```

#### Sprint 2: Testes B√°sicos
```markdown
- [ ] 150+ testes unit√°rios
- [ ] 5-7 m√≥dulos cr√≠ticos cobertos
- [ ] Cobertura >= 70%
- [ ] Coverage reporting configurado
- [ ] Tempo de execu√ß√£o < 1 minuto
- [ ] TESTING.md documentado
```

#### Sprint 3: Integra√ß√£o + CLI
```markdown
- [ ] Testes de integra√ß√£o consolidados
- [ ] CLI com 8-10 comandos
- [ ] 5-10 testes E2E
- [ ] CLI documentada
- [ ] Testes CLI criados
```

#### Sprint 4: Cobertura + CI
```markdown
- [ ] 80-120 novos testes
- [ ] 3-5 m√≥dulos secund√°rios cobertos
- [ ] CI/CD configurado
- [ ] Jobs separados (unit, lint, coverage)
- [ ] Coverage threshold enforced
- [ ] Badges no README
```

#### Sprint 5: Qualidade Final
```markdown
- [ ] 100% servi√ßos com testes
- [ ] Cobertura >= 85%
- [ ] Framework de performance
- [ ] Benchmarks documentados
- [ ] QUALITY_REPORT.md
```

#### Sprint 6: Observabilidade
```markdown
- [ ] Logging estruturado (JSON + console)
- [ ] Request ID implementado
- [ ] /metrics endpoint
- [ ] Middleware de logging
- [ ] OBSERVABILITY.md
```

#### Sprint 7: Seguran√ßa Base
```markdown
- [ ] Rate limiting implementado
- [ ] Audit logging implementado
- [ ] RBAC funcional
- [ ] 0 secrets hardcoded
- [ ] 40+ testes de seguran√ßa
- [ ] SECURITY.md
```

#### Sprint 8: Hardening
```markdown
- [ ] Rate limiting distribu√≠do (Redis)
- [ ] RBAC granular (ownership)
- [ ] CI/CD security gates
- [ ] M√©tricas de seguran√ßa
- [ ] Runbook inicial
```

#### Sprint 9: Ops + DR
```markdown
- [ ] MFA/2FA implementado
- [ ] Scripts backup/restore
- [ ] Trace ID implementado
- [ ] Runbook 500+ linhas
- [ ] Security scans executados
- [ ] 0 vulnerabilidades cr√≠ticas
- [ ] Production-ready >= 98%
```

---

## üõ†Ô∏è Ferramentas e Recursos

### Ferramentas por Categoria

#### Testes
- **Python:** pytest, pytest-cov, pytest-mock, freezegun
- **JavaScript:** jest, vitest, mocha, chai
- **Java:** JUnit, TestNG, Mockito
- **Go:** testing package, testify

#### Observabilidade
- **Logging:** structlog (Python), winston/pino (JS), logback (Java), zap/logrus (Go)
- **M√©tricas:** Prometheus, StatsD, OpenTelemetry
- **Tracing:** Jaeger, Zipkin, OpenTelemetry
- **Monitoramento:** Grafana, Datadog, New Relic

#### Seguran√ßa
- **SAST:** Bandit (Python), ESLint security, SpotBugs (Java), gosec (Go)
- **Dependency Scan:** Safety, pip-audit (Python), npm audit (JS), OWASP Dependency Check
- **Secret Scan:** TruffleHog, git-secrets, detect-secrets
- **Penetration Test:** OWASP ZAP, Burp Suite

#### CI/CD
- **Plataformas:** GitHub Actions, GitLab CI, Jenkins, CircleCI
- **Coverage:** Codecov, Coveralls, SonarQube
- **Badges:** Shields.io

#### Infraestrutura
- **Containers:** Docker, Podman
- **Orquestra√ß√£o:** Kubernetes, Docker Compose, Nomad
- **Secrets:** Vault, AWS Secrets Manager, Azure Key Vault

### Recursos de Aprendizado

#### Documenta√ß√£o Oficial
- [Pytest Documentation](https://docs.pytest.org/)
- [Structlog Documentation](https://www.structlog.org/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

#### Guias e Tutoriais
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [12 Factor App](https://12factor.net/)
- [SRE Books (Google)](https://sre.google/books/)

---

## üîß Solu√ß√£o de Problemas

### Problema 1: IA Gera C√≥digo Muito Gen√©rico

**Sintomas:**
- C√≥digo com muitos coment√°rios "# TODO"
- Pseudoc√≥digo em vez de implementa√ß√£o real
- Falta de integra√ß√£o com c√≥digo existente

**Solu√ß√µes:**
1. ‚úÖ Adicione mais contexto ao prompt (stack, framework, vers√µes)
2. ‚úÖ Forne√ßa exemplos de c√≥digo existente
3. ‚úÖ Seja espec√≠fico sobre o que voc√™ quer (n√£o "criar testes", mas "criar 40 testes pytest para auth_service.py")
4. ‚úÖ Pe√ßa c√≥digo execut√°vel, n√£o exemplos

---

### Problema 2: Testes Gerados Falham

**Sintomas:**
- Testes n√£o passam ao executar
- Imports incorretos
- Mocks n√£o configurados

**Solu√ß√µes:**
1. ‚úÖ Revise e ajuste imports manualmente
2. ‚úÖ Verifique se mocks est√£o configurados corretamente
3. ‚úÖ Execute testes incrementalmente (1-2 por vez)
4. ‚úÖ Pe√ßa para IA revisar testes falhando (forne√ßa erro)

---

### Problema 3: C√≥digo N√£o Segue Padr√µes do Projeto

**Sintomas:**
- Estilo diferente (camelCase vs snake_case)
- Estrutura de arquivos n√£o alinhada
- Conven√ß√µes de nomenclatura diferentes

**Solu√ß√µes:**
1. ‚úÖ Inclua exemplos de c√≥digo existente no prompt
2. ‚úÖ Especifique padr√µes explicitamente (PEP 8, Airbnb style, etc.)
3. ‚úÖ Use linters para validar (pylint, eslint)
4. ‚úÖ Pe√ßa para IA refatorar seguindo padr√µes

---

### Problema 4: Muitas Mudan√ßas de Uma Vez

**Sintomas:**
- Dif√≠cil de revisar
- N√£o sabe o que mudou
- Risco de quebrar algo

**Solu√ß√µes:**
1. ‚úÖ Divida sprint em tarefas menores
2. ‚úÖ Aplique mudan√ßas incrementalmente
3. ‚úÖ Execute testes ap√≥s cada mudan√ßa
4. ‚úÖ Use controle de vers√£o (commits pequenos)

---

### Problema 5: CI/CD Quebrando Ap√≥s Sprint

**Sintomas:**
- Pipeline falhando
- Testes passam localmente mas falham no CI
- Coverage abaixo do threshold

**Solu√ß√µes:**
1. ‚úÖ Execute CI localmente antes (act para GitHub Actions)
2. ‚úÖ Verifique depend√™ncias (requirements.txt atualizado?)
3. ‚úÖ Revise configura√ß√£o do CI (.github/workflows/)
4. ‚úÖ Ajuste threshold de coverage se necess√°rio

---

## üéì Conclus√£o

Este playbook documenta um m√©todo comprovado para evoluir projetos de software usando IA. A chave do sucesso √©:

1. **Dividir em Sprints:** Incrementos pequenos e valid√°veis
2. **Prompts Eficazes:** Contexto, objetivo, restri√ß√µes, m√©tricas
3. **Valida√ß√£o Cont√≠nua:** Testes, scans, reviews
4. **Documenta√ß√£o Viva:** Relat√≥rios de sprint, aprendizados
5. **IA como Parceira:** IA sugere, humano valida e refina

### Pr√≥ximos Passos

1. **Escolha Sua Sprint:** Comece com Sprint 1 (Estrutura) se novo, ou Sprint 6 (Observabilidade) se c√≥digo j√° testado
2. **Adapte os Prompts:** Use AI-SPRINT-PROMPTS.md como base
3. **Execute e Valide:** Siga checklists de valida√ß√£o
4. **Documente:** Crie relat√≥rios de sprint
5. **Itere:** Refine prompts baseado em resultados

### Recursos Adicionais

- **Framework Completo:** [AI-SPRINT-FRAMEWORK.md](./AI-SPRINT-FRAMEWORK.md)
- **Prompts Reutiliz√°veis:** [AI-SPRINT-PROMPTS.md](./AI-SPRINT-PROMPTS.md)
- **Exemplo Real:** Sprints 1-9 do projeto 3dPot

---

**Vers√£o:** 1.0  
**√öltima Atualiza√ß√£o:** Novembro 2025  
**Baseado em:** 3dPot Sprints 1-9  
**Status:** Production-Ready Playbook

---

## üìû Feedback e Contribui√ß√µes

Este playbook √© um documento vivo. Se voc√™ usar este m√©todo em seu projeto:

1. **Compartilhe resultados:** M√©tricas antes/depois
2. **Documente ajustes:** Prompts que funcionaram melhor
3. **Relate problemas:** Desafios encontrados e solu√ß√µes
4. **Sugira melhorias:** Novas sprints, ferramentas, pr√°ticas

**Happy Engineering with AI! üöÄ**
